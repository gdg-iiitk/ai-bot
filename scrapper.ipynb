{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Tutorial with BeautifulSoup\n",
    "\n",
    "This notebook demonstrates web scraping using BeautifulSoup to:\n",
    "1. Map all endpoints of a website\n",
    "2. Scrape content by tags, classes, and IDs\n",
    "3. Save scraped data to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install beautifulsoup4 requests ratelimit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to scrape https://iiitkottayam.ac.in/\n",
      "Found 5 endpoints\n",
      "\n",
      "Scraping: /data/pdf/cvo_new.pdf\n",
      "\n",
      "Scraping: /data/pdf/OM-formation of SC-ST-OBC Cell-Jan2024.pdf\n",
      "\n",
      "Scraping: /data/pdf/recruiterscorner.pdf\n",
      "\n",
      "Scraping: /data/pdf/OM-Disciplinary Action Committee-revised-Jan2024.pdf\n",
      "\n",
      "Scraping: /\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "class WebScraper:\n",
    "    def __init__(self, base_url, rate_limit=1):\n",
    "        self.base_url = base_url.rstrip('/')\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "        self.rate_limit = rate_limit\n",
    "    \n",
    "    @sleep_and_retry\n",
    "    @limits(calls=1, period=1)  # 1 call per second\n",
    "    def get_soup(self, endpoint, timeout=10):\n",
    "        url = urljoin(self.base_url, endpoint)\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=timeout)\n",
    "            response.raise_for_status()\n",
    "            return BeautifulSoup(response.text, 'html.parser')\n",
    "        except RequestException as e:\n",
    "            print(f\"Error fetching {url}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_elements(self, endpoint, **kwargs):\n",
    "        soup = self.get_soup(endpoint)\n",
    "        if not soup:\n",
    "            return []\n",
    "            \n",
    "        elements = soup.find_all(**kwargs) if kwargs.get('find_all', True) else [soup.find(**kwargs)]\n",
    "        return [elem.text.strip() for elem in elements if elem]\n",
    "\n",
    "    def scrape_by_tag(self, tag, endpoint):\n",
    "        return self.scrape_elements(endpoint, name=tag)\n",
    "    \n",
    "    def scrape_by_class(self, class_name, endpoint):\n",
    "        return self.scrape_elements(endpoint, class_=class_name)\n",
    "    \n",
    "    def scrape_by_id(self, id_name, endpoint):\n",
    "        return self.scrape_elements(endpoint, id=id_name, find_all=False)\n",
    "    \n",
    "    def save_to_file(self, content, filename):\n",
    "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            for item in content:\n",
    "                f.write(f\"{item}\\n{'='*80}\\n\")\n",
    "\n",
    "class SiteMapper:\n",
    "    def __init__(self, scraper):\n",
    "        self.scraper = scraper\n",
    "        self.visited = set()\n",
    "        self.domain = urlparse(scraper.base_url).netloc\n",
    "    \n",
    "    def is_valid_url(self, url):\n",
    "        parsed = urlparse(url)\n",
    "        return parsed.netloc == self.domain\n",
    "    \n",
    "    def map_site(self, start_path, max_depth=2):\n",
    "        if max_depth <= 0 or start_path in self.visited:\n",
    "            return set()\n",
    "        \n",
    "        self.visited.add(start_path)\n",
    "        soup = self.scraper.get_soup(start_path)\n",
    "        if not soup:\n",
    "            return {start_path}\n",
    "            \n",
    "        endpoints = {start_path}\n",
    "        \n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if not href or href.startswith(('#', 'javascript:', 'mailto:', 'tel:')):\n",
    "                continue\n",
    "                \n",
    "            full_url = urljoin(self.scraper.base_url, href)\n",
    "            if self.is_valid_url(full_url):\n",
    "                path = urlparse(full_url).path or '/'\n",
    "                if path not in self.visited:\n",
    "                    endpoints.update(self.map_site(path, max_depth - 1))\n",
    "        \n",
    "        return endpoints\n",
    "\n",
    "def scrape_full_site(base_url, max_depth=2):\n",
    "    scraper = WebScraper(base_url)\n",
    "    mapper = SiteMapper(scraper)\n",
    "    \n",
    "    print(f\"Starting to scrape {base_url}\")\n",
    "    endpoints = mapper.map_site(\"/\", max_depth=max_depth)\n",
    "    print(f\"Found {len(endpoints)} endpoints\")\n",
    "    \n",
    "    for endpoint in endpoints:\n",
    "        print(f\"\\nScraping: {endpoint}\")\n",
    "        safe_endpoint = endpoint.replace('/', '_').lstrip('_')\n",
    "        if not safe_endpoint:\n",
    "            safe_endpoint = 'home'\n",
    "            \n",
    "        try:\n",
    "            # Scrape and save paragraphs\n",
    "            paragraphs = scraper.scrape_by_tag(\"p\", endpoint)\n",
    "            if paragraphs:\n",
    "                scraper.save_to_file(paragraphs, f\"data/paragraphs/{safe_endpoint}.txt\")\n",
    "            \n",
    "            # Scrape and save content divs\n",
    "            content = scraper.scrape_by_class(\"content\", endpoint)\n",
    "            if content:\n",
    "                scraper.save_to_file(content, f\"data/content/{safe_endpoint}.txt\")\n",
    "            \n",
    "            # Scrape and save main content\n",
    "            main_content = scraper.scrape_by_id(\"main\", endpoint)\n",
    "            if main_content:\n",
    "                scraper.save_to_file(main_content, f\"data/main/{safe_endpoint}.txt\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {endpoint}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "# Initialize and run\n",
    "BASE_URL = \"https://iiitkottayam.ac.in/\"\n",
    "scrape_full_site(BASE_URL, max_depth=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
